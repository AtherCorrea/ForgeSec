## Packet Capture and Ingestion

This is the **first stage** of the IDS/IPS engine the moment when raw packets are received from the network interface and made available for inspection.

# Raw Packets

### 📘 What Is It?

Raw packets are **complete network frames** captured directly from the Network Interface Card (NIC), before any operating system or application-level processing occurs.

They preserve the full structure of the packet, including:

- **Layer 2 – Data Link**: MAC source and destination addresses (Ethernet header)  
- **Layer 3 – Network**: IP headers, TTL, protocol type  
- **Layer 4 – Transport**: TCP/UDP ports, flags like SYN, ACK, FIN  
- **Layer 7 – Application**: HTTP requests, DNS queries, payload data  

This raw form enables **low-level traffic inspection** with zero abstraction essential in cybersecurity where every byte matters.

Tools like **Suricata**, **tcpdump**, and **Wireshark** rely on raw packets to analyze traffic, detect threats, discover scans, and identify evasion techniques or malformed payloads.

### 🔎 Practical Example

When you run the command below:

`tcpdump -i eth0 -XX`

You're viewing **raw packets**, arriving at your interface byte-by-byte — including fields and headers the system would normally abstract away.

Example of raw content you might see:

`GET / HTTP/1.1`  
`Host: google.com`

And before that, in the headers:

- Source and destination IPs  
- TCP source/destination ports  
- Flags like `SYN` (indicating new connection)  
- Ethernet MAC addresses  

An IDS like **Suricata** connects to this same layer, pulling packets directly from the NIC and inspecting them using detection rules to identify:

- Port scans (e.g., Nmap)  
- Malicious traffic patterns  
- Protocol exploits and evasions  
- Obfuscated or malformed packets  

### 🧠 Why This Matters

True network security starts at the **raw packet level**. Once a packet is processed by the OS, it may be **altered, normalized, or dropped**  hiding vital clues.

With raw packets, you analyze **what truly arrived on the wire**, unaffected by the system. IDS tools need this to detect subtle or evasive threats with precision.

### 📚 Further Reference

- [RFC 791 – Internet Protocol (IP)](https://datatracker.ietf.org/doc/html/rfc791)  
- [Wireshark – Packet Structure Overview](https://wiki.wireshark.org/CaptureSetup/Ethernet)  
- [tcpdump – Raw Packet Capture](https://www.tcpdump.org/manpages/tcpdump.1.html)

# Libpcap

### 📘 What Is It?

**libpcap** (Packet Capture Library) is a low-level C/C++ library that provides an interface for capturing raw network packets directly from the **Network Interface Card (NIC)**.

It allows tools to observe all traffic reaching the NIC, regardless of whether the packets are destined for the host machine this is known as **promiscuous mode**.

Key responsibilities of libpcap:

- Access raw packets from the NIC (layer 2)
- Apply **BPF (Berkeley Packet Filter)** rules to capture only relevant traffic
- Attach **timestamps** and **packet metadata**
- Deliver packets to user-space tools for analysis

It serves as the foundation for many essential tools in networking and cybersecurity:

- `tcpdump`
- `Wireshark`
- `Snort`
- `Suricata` (when using `pcap` mode)

On Linux, `libpcap` works through OS-level interfaces like **AF_PACKET**, **PF_PACKET**, or **BPF**, depending on the system and configuration.

---

### 🔎 Practical Example

 Suppose you want to capture all HTTP traffic on the interface `eth0`.

You run:

`tcpdump -i eth0 tcp port 80`

What happens under the hood:

- `tcpdump` calls `libpcap_open_live("eth0", ...)`
- libpcap sets the interface to **promiscuous mode**
- A **BPF filter** is installed: `tcp port 80`
- libpcap reads packets from the kernel buffer and passes them to `tcpdump`

You start seeing HTTP requests, responses, and TCP handshakes all captured directly from the wire.

In Suricata, libpcap is used in two key modes:

1. **Live capture mode**:  
   Suricata connects directly to an interface using `pcap` to inspect real-time traffic.

2. **Offline mode**:  
   Suricata analyzes a previously recorded `.pcap` file, such as:

   `suricata -r capture.pcap -c suricata.yaml`

This enables retrospective analysis or reproducing attack scenarios in your lab.

### 📚 Further Reference

- [tcpdump/libpcap official page](https://www.tcpdump.org/)
- [pcap(3) man page – Linux](https://linux.die.net/man/3/pcap)
- [BPF Filter Syntax (man page)](https://man7.org/linux/man-pages/man7/pcap-filter.7.html)
- [Wireshark – How libpcap works](https://wiki.wireshark.org/CaptureSetup/LibpcapFileFormat)

## BPF (Berkeley Packet Filter)

### 📘 What Is It?

BPF (Berkeley Packet Filter) is a kernel-level filtering mechanism used to efficiently discard unwanted network packets **before** they reach userspace. It acts as a programmable filter on the NIC interface, selecting which packets a tool like `tcpdump`, `Wireshark`, or Suricata should actually see.

BPF has two main components:

- A **human-readable filtering syntax**, like:
  - `tcp port 80`
  - `udp port 53`
  - `host 192.168.1.1`

- A **bytecode-based virtual machine inside the kernel**, which compiles and runs those filters at high speed.

This design allows extremely fast filtering with minimal CPU usage, especially useful in high-throughput environments.

> BPF does the filtering inside the kernel, while tools like `libpcap` act as a bridge between BPF and user applications.

---

### How It Works

When an application defines a BPF filter using `libpcap`, this is what happens internally:

1. The human-readable filter (e.g., `tcp port 80`) is compiled into BPF bytecode.
2. This bytecode is installed in the kernel via `setsockopt()` or equivalent.
3. The kernel runs the filter logic for every incoming packet.
4. Only the matching packets are sent to the userspace application.

This means tools process *only what they need*, saving CPU and memory.

---

### 🔎 Practical Example

Run the following:

```
tcpdump -i eth0 tcp port 80
```

Under the hood:

- `tcpdump` calls `libpcap`, which compiles the string filter to BPF bytecode.
- The kernel applies that filter to traffic on `eth0`.
- Only packets with destination or source TCP port 80 are captured.

Typical output includes:

```
15:12:03.248739 IP 192.168.0.2.443 > 192.168.0.10.57821: Flags [P.], length 512
```

---

### In Suricata

When Suricata is configured to use `libpcap` in live capture mode, you can define BPF filters in the config file:

```yaml
pcap:
  - interface: eth0
    bpf-filter: "tcp port 80 or port 443"
```

This setup ensures Suricata only receives HTTP and HTTPS traffic. All other packets are dropped at the kernel level, saving resources and improving performance.

---

### Timestamps and Packet Metadata

When a packet passes through BPF and is delivered to a tool like Suricata or `tcpdump`, `libpcap` attaches metadata such as:

| Metadata          | Example                          | Description                          |
|------------------|----------------------------------|--------------------------------------|
| 🕒 Timestamp       | `2025-06-24 15:12:03.248739`     | Exact time of capture (µs or ns)     |
| 📏 Captured size   | `96 bytes`                      | Bytes actually captured               |
| 📦 Original size   | `1514 bytes`                    | Full size on the wire                |
| 🌐 Interface       | `eth0`, `ens33`                 | Which NIC received the packet        |

This data helps with:

- Reordering packets
- Measuring latency
- Replaying traffic
- Correlating with logs and alerts

---

### BPF vs libpcap

| Feature                     | `libpcap`        | `BPF` (kernel)            |
|----------------------------|------------------|---------------------------|
| Userspace API              | ✅ Yes           | ❌ No                     |
| Runs in kernel             | ❌ No            | ✅ Yes                    |
| Filtering engine           | Uses BPF         | Is the engine             |
| Filter expression parsing  | ✅ (string to bytecode) | ❌ (executes only)  |
| Drops unwanted traffic     | ❌ No            | ✅ Yes (very early)       |

---

### 📚 Further Reference

- [pcap-filter(7) – Official man page](https://man7.org/linux/man-pages/man7/pcap-filter.7.html)
- [Classic BPF Paper – McCanne & Jacobson (1993)](https://www.tcpdump.org/papers/bpf-usenix93.pdf)
- [Suricata and libpcap integration](https://docs.suricata.io/en/latest/capture-hardware/pcap.html)
- [Introduction to BPF | LINUX Berkeley Packet Filter](https://www.youtube.com/watch?v=qXFi-G_7IuU)
- [eBPF – Next-generation packet filtering](https://ebpf.io/)

# AF_PACKET

#### 📘 What Is It?

**AF_PACKET** is a Linux-specific socket type that allows programs to capture raw packets directly from the NIC at Layer 2. Unlike libpcap, which operates entirely in user space, AF_PACKET interacts more closely with the Linux kernel, offering higher performance and lower latency.

It supports:

- Zero-copy capture via TPACKET_V3  
- Ring buffers for efficient memory usage  
- Capture of inbound and outbound packets  
- Better performance for high-speed traffic

AF_PACKET is commonly used in Suricata deployments that require real-time packet analysis without the overhead of traditional pcap.

#### 🔎 Practical Example

Suppose Suricata is using AF_PACKET. It connects directly to `eth0`, maps a ring buffer, and begins processing packets without extra copying.

Basic snippet in `suricata.yaml` would look like:

af-packet:  

- interface: eth0  
  threads: auto  
  cluster-id: 99  
  defrag: yes

This mode is ideal for high-throughput environments like gigabit+ networks.

#### 📚 Further Reference

- [Linux Kernel AF_PACKET Guide](https://www.kernel.org/doc/html/latest/networking/af_packet.html)  
- [Suricata AF_PACKET Documentation](https://docs.suricata.io/en/latest/capture-hardware/af-packet.html)

# Netmap

## What Is It?

Netmap is a high-performance framework designed for direct packet I/O between the NIC (Network Interface Card) and user-space applications, bypassing most of the kernel’s network stack.

It was developed to overcome the performance bottlenecks found in traditional capture methods like `libpcap` and `AF_PACKET`, enabling applications to handle millions of packets per second with extremely low latency.

Netmap is particularly useful for:

- High-performance IDS/IPS systems like Suricata  
- Embedded firewalls or routers  
- Custom network appliances  
- High-volume traffic analysis tools

## How It Works

Instead of following the standard Linux packet path (NIC → driver → kernel stack → libpcap), Netmap allocates shared memory buffers (rings) directly accessible from both the NIC and user space using DMA (Direct Memory Access).

This results in:

- Zero-copy packet transfers  
- Direct access to NIC buffers  
- Minimal kernel involvement  
- Reduced CPU usage and overhead  

This architecture provides very high-speed packet processing, often exceeding 10 Gbps per core on modern hardware.

### Traditional Path vs Netmap

| Traditional (libpcap)                        | Netmap Path                          |
|---------------------------------------------|--------------------------------------|
| NIC → Driver → Kernel → libpcap             | NIC → Netmap → Application           |
| Kernel involved in each packet              | Kernel bypass                        |
| High overhead and latency                   | Minimal overhead, maximum performance |

## Practical Example: Using Netmap with Suricata

To enable Netmap in Suricata, configure the interface in `suricata.yaml`:

netmap:

```
interface: netmap:eth0
threads: auto
copy-mode: false
disable-promisc: false
checksum-checks: auto
```

Important notes:

- The interface must be prefixed with `netmap:`  
- On some systems, Netmap requires compiling a kernel module (`netmap.ko`)  
- Hardware and driver compatibility is required  

## Real-World Performance Comparison

| Capture Method | Throughput (1 core)     | Copy to User Space | Efficiency (64B pkts) |
|----------------|--------------------------|--------------------|------------------------|
| libpcap        | ~100 Mbps                | Yes (high latency) | Low                    |
| AF_PACKET      | ~500 Mbps – 1 Gbps       | Yes                | Medium                 |
| Netmap         | 10 Gbps+                 | No (zero-copy)     | High                   |

Netmap has been benchmarked at over 14 million packets per second per CPU core.

## Advanced Features

Netmap supports:

- VALE: a virtual switch connecting multiple Netmap applications  
- Bridge and router emulation in user space  
- Passthrough mode for inline IPS  
- Parallel interface capture and distribution  

## Requirements and Limitations

| Requirement                     | Details                                                                 |
|--------------------------------|-------------------------------------------------------------------------|
| Kernel patch or compatible driver | Some NICs require Netmap-aware drivers or patched kernels              |
| Dedicated interface             | Incompatible with standard OS TCP/IP stack                             |
| No loopback support             | Cannot capture traffic from the local machine                          |
| Low-level configuration needed  | Requires knowledge of rings, buffers, and Netmap APIs                  |

## When to Use It

| Scenario                                      | Use Netmap? | Justification                                           |
|----------------------------------------------|-------------|---------------------------------------------------------|
| Homelab or small-scale testing               | No          | Overkill; AF_PACKET is more than sufficient             |
| High-throughput IDS in production            | Yes         | Improves accuracy and minimizes latency                 |
| Multi-gigabit capture with multiple cores    | Yes         | One of the best open solutions for packet processing    |
| Forensic analysis on .pcap files             | No          | Use `libpcap` or replay tools for simplicity            |

## Further Reference

- [Netmap Project – Official Site](https://info.iet.unipi.it/~luigi/netmap/)  
- [Original Netmap Paper – Luigi Rizzo (ACM)](https://ieeexplore.ieee.org/document/6232970)  
- [Suricata Netmap Mode – Official Docs](https://docs.suricata.io/en/latest/capture-hardware/netmap.html)  
- [Benchmark: Netmap vs DPDK vs PF_RING](https://www.slideshare.net/napira/netmap-vs-dpdk-vs-pf-ring)

# DPDK

#### 📘 What Is It?

**DPDK (Data Plane Development Kit)** is a set of libraries and drivers that allow user-space applications to perform extremely fast packet processing by bypassing the Linux kernel entirely.

It allows:

- Direct NIC access via DMA (no interrupts or syscalls)  
- Pinning processes to CPU cores for deterministic performance  
- Processing traffic at 10/40/100 Gbps with low latency

DPDK is used in production-grade network systems like telecom firewalls, commercial IDS/IPS, and SDN platforms.

#### 🔎 Practical Example

With DPDK, Suricata binds directly to the NIC using `vfio-pci`. You may configure boot parameters (like hugepages) and allocate dedicated cores.

This setup allows:

- Real-time processing of high-speed traffic  
- Packet batching for efficient CPU cache usage  
- Bypass of OS network stack

It requires detailed setup, including driver binding and NIC configuration.

#### 📚 Further Reference

- [DPDK Official Documentation](https://doc.dpdk.org/guides/)  
- [Suricata with DPDK](https://docs.suricata.io/en/latest/capture-hardware/dpdk.html)

# SPAN Port vs Inline Bridge

#### 📘 What Is It?

These are two main modes of how an IDS/IPS observes or controls traffic:

**SPAN Port (Passive Monitoring):**

- Switch copies traffic from target ports to a monitoring port  
- IDS receives traffic without interfering  
- Safe for analysis and learning  
- Cannot block or drop malicious traffic

**Inline Bridge (Active Mode):**

- IDS sits between two network points (like a firewall)  
- Traffic flows through it in real time  
- Can block, reject, or modify traffic based on rules  
- Enables Intrusion Prevention capabilities

#### 🔎 Practical Example

**SPAN Port**:  
You configure a switch to mirror traffic from a server to a port where Suricata is listening. It observes traffic but doesn't interfere. Ideal for lab setups.

**Inline Bridge**:  
Suricata is placed physically or logically between the router and LAN. All traffic flows through it. If malicious traffic is detected, it can be dropped or modified.

This method offers full control but requires more care to avoid disrupting network flow.

#### 📚 Further Reference

- [SPAN vs TAP vs Inline](https://www.ixiacom.com/company/blog/span-vs-tap-what-difference)  
- [Suricata Inline IPS Guide](https://docs.suricata.io/en/latest/configuration/suricata-yaml.html#inline-mode)
